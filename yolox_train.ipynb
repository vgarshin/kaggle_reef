{"cells":[{"cell_type":"markdown","metadata":{"id":"oH7lM_mhLlIS"},"source":["# YOLOX on COTS dataset: train"],"id":"oH7lM_mhLlIS"},{"cell_type":"code","execution_count":null,"metadata":{"id":"QjIR8BqZLlIa"},"outputs":[],"source":["DEBUG = False\n","COLAB = True\n","VER = 'vclbyolo30'\n","WORK_DIR = '/content/drive/MyDrive/reef' if COLAB else '/u01/mrorange/reef'"],"id":"QjIR8BqZLlIa"},{"cell_type":"markdown","metadata":{"id":"O0IJ_gFoLlIg"},"source":["## Install YOLOX"],"id":"O0IJ_gFoLlIg"},{"cell_type":"code","execution_count":null,"metadata":{"id":"8CECnsxJycd7"},"outputs":[],"source":["if COLAB:\n","    from google.colab import drive\n","    drive.mount('/content/drive')"],"id":"8CECnsxJycd7"},{"cell_type":"code","execution_count":null,"metadata":{"id":"GIZVsXT6LlIh"},"outputs":[],"source":["if COLAB:\n","    %cd {WORK_DIR}\n","    !git clone https://github.com/Megvii-BaseDetection/YOLOX -q\n","    %cd YOLOX\n","    !pip install -U pip && pip install -r requirements.txt\n","    !pip install -v -e . \n","    !pip install 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'"],"id":"GIZVsXT6LlIh"},{"cell_type":"code","execution_count":null,"metadata":{"id":"IyvYw02PLlId"},"outputs":[],"source":["import ast\n","import os\n","import json\n","import time\n","import random\n","import numpy as np\n","import pandas as pd\n","import torch\n","import importlib\n","import cv2 \n","from shutil import copyfile\n","from tqdm.notebook import tqdm\n","tqdm.pandas()\n","from sklearn.model_selection import GroupKFold\n","from PIL import Image\n","from string import Template\n","from IPython.display import display\n","import warnings\n","if DEBUG:\n","    warnings.filterwarnings('ignore', category=UserWarning)"],"id":"IyvYw02PLlId"},{"cell_type":"code","execution_count":null,"metadata":{"id":"ETzu5w7jLlIf"},"outputs":[],"source":["CONFIG = {\n","    'ver': VER,\n","    'bbone': 'yolox_l.pth', # yolox_s.pth yolox_nano.pth\n","    'width': 1280, \n","    'height': 720,\n","    'resize': (1920, 1920), # 460 640 960 1024 (1280, 736) (1280, 1280)\n","    'batch_size': 2, # 8\n","    'workers': 4 if COLAB else 8,\n","    'folds': 5,\n","    'bal_split': None, #'train_split_balanced_v1', None\n","    'val_fold': 4,\n","    'val_video': 3, # 1, 2, 3\n","    'empty_sh': .2,\n","    'test_conf': .1, # default .01\n","    'nmsthre': .65, # default .65\n","    'basic_lr_per_img': .01 / 64, # default .01 / 64\n","    'random_size': (40, 61), # default (14, 26)\n","    'mosaic_prob': 0, # default 1.0\n","    'mixup_prob': 0.5, # default 1.0\n","    'hsv_prob': 0.5, # default 1.0\n","    'flip_prob': 0.5, # default 0.5\n","    'degrees': 0, # default 10.0\n","    'translate': 0.1, # default 0.1\n","    'mosaic_scale': (0.6, 1.4), # default (0.1, 2)\n","    'mixup_scale': (0.5, 1.5), # default (0.5, 1.5)\n","    'shear': 2.0, # default 2.0\n","    'enable_mixup': True, # default True\n","    'warmup_epochs': 2,\n","    'no_aug_epochs': 0,\n","    'epochs': 4 if DEBUG else 8,\n","    'seed': 2022\n","}\n","VER_DATA = f'fs{CONFIG[\"folds\"]}vf{CONFIG[\"val_fold\"]}'\n","DATA_PATH = f'{WORK_DIR}/data'\n","if CONFIG[\"bal_split\"]:\n","    YDATA_PATH = f'{WORK_DIR}/data_bs_{VER_DATA}'\n","elif CONFIG['val_video']:\n","    YDATA_PATH = f'{WORK_DIR}/data_vv_{CONFIG[\"val_video\"] - 1}'\n","else:\n","     YDATA_PATH = f'{WORK_DIR}/data_{VER_DATA}'\n","print('data path:', YDATA_PATH)\n","MDLS_PATH = f'{WORK_DIR}/models_{VER}'\n","TH = CONFIG['test_conf']\n","NMS_TH = CONFIG['nmsthre']\n","NUM_CLASSES = 1\n","COCO_CLASSES = (\n","  'starfish',\n",")\n","PIPELINE_CONFIG_PATH = f'cots_config_{VER}.py'\n","if not os.path.exists(YDATA_PATH):\n","    os.mkdir(YDATA_PATH)\n","if not os.path.exists(MDLS_PATH):\n","    os.mkdir(MDLS_PATH)\n","with open(f'{MDLS_PATH}/config.json', 'w') as file:\n","    json.dump(CONFIG, file)\n","\n","def seed_all(seed=0):\n","    np.random.seed(seed)\n","    random_state = np.random.RandomState(seed)\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    return random_state    \n","    \n","random_state = seed_all(CONFIG['seed'])\n","start_time = time.time()"],"id":"ETzu5w7jLlIf"},{"cell_type":"markdown","metadata":{"id":"kTxyXduiLlIi"},"source":["## Data preprocessing"],"id":"kTxyXduiLlIi"},{"cell_type":"code","execution_count":null,"metadata":{"id":"dJsQMYnCLlIj"},"outputs":[],"source":["def get_bbox(anns):\n","    bboxes = [list(ann.values()) for ann in anns]\n","    return bboxes\n","\n","def get_path(row):\n","    row['image_path'] = f'{DATA_PATH}/train_images/video_{row.video_id}/{row.video_frame}.jpg'\n","    return row\n","\n","def df_proc(df):\n","    df['annotations'] = df['annotations'].progress_apply(lambda x: ast.literal_eval(x))\n","    df['bboxes'] = df.annotations.progress_apply(get_bbox)\n","    df[\"width\"] = CONFIG['width']\n","    df[\"height\"] = CONFIG['height']\n","    df = df.progress_apply(get_path, axis=1)\n","    return df"],"id":"dJsQMYnCLlIj"},{"cell_type":"code","execution_count":null,"metadata":{"id":"M0ucZ3XeLlIk"},"outputs":[],"source":["df = pd.read_csv(f'{DATA_PATH}/{CONFIG[\"bal_split\"]}.csv') if CONFIG[\"bal_split\"] else pd.read_csv(f'{DATA_PATH}/train.csv') \n","df[\"num_bbox\"] = df['annotations'].apply(lambda x: str.count(x, 'x'))\n","df = df[df[\"num_bbox\"] > 0]\n","df = df_proc(df)\n","\n","df_ = pd.read_csv(f'{DATA_PATH}/train.csv') \n","df_[\"num_bbox\"] = df_['annotations'].apply(lambda x: str.count(x, 'x'))\n","df_ = df_[df_[\"num_bbox\"] == 0]\n","df_ = df_.sample(int(CONFIG['empty_sh'] * len(df)))\n","df_.reset_index(inplace=True)\n","df_ = df_proc(df_)\n","\n","gkf = GroupKFold(n_splits=CONFIG['folds'])\n","if CONFIG[\"bal_split\"]:\n","    df['fold'] = df['fold_id']\n","    df_['fold'] = -1\n","    for i, (train_idxs, val_idxs) in enumerate(gkf.split(df_, groups=df_['sequence'])):\n","        df_.loc[val_idxs, 'fold'] = i\n","    df = df.append(df_)\n","else:\n","    df = df.append(df_)\n","    if DEBUG: \n","        df = df.sample(100)\n","    df = df.reset_index(drop=True)\n","    df['fold'] = -1\n","    for i, (train_idxs, val_idxs) in enumerate(gkf.split(df, groups=df['sequence'])):\n","        df.loc[val_idxs, 'fold'] = i\n","    if CONFIG['val_video']:\n","        df['fold'] = df['video_id']\n","display(df.head())"],"id":"M0ucZ3XeLlIk"},{"cell_type":"code","execution_count":null,"metadata":{"id":"cGPddw2VAGyO"},"outputs":[],"source":["df.groupby(by=['video_id', 'sequence', 'fold']).sum()['num_bbox']"],"id":"cGPddw2VAGyO"},{"cell_type":"markdown","metadata":{"id":"xd6bpcWoLlIn"},"source":["## Annotation files and image folders"],"id":"xd6bpcWoLlIn"},{"cell_type":"code","execution_count":null,"metadata":{"id":"V28f2tr6LlIo"},"outputs":[],"source":["def save_ann_json(json_ann, filename):\n","    with open(filename, 'w') as file:\n","        output_json = json.dumps(json_ann)\n","        file.write(output_json)\n","\n","def dataset2coco(df, dest_path):\n","    global ann_id\n","    anns_json = {\n","        \"info\": [],\n","        \"licenses\": [],\n","        \"categories\": [],\n","        \"images\": [],\n","        \"annotations\": []\n","    }\n","    info = {\n","        \"year\": \"2021\",\n","        \"version\": \"1\",\n","        \"description\": \"COTS dataset - COCO format\",\n","        \"contributor\": \"\",\n","        \"url\": \"https://kaggle.com\",\n","        \"date_created\": \"2021-11-30T15:01:26+00:00\"\n","    }\n","    anns_json[\"info\"].append(info)\n","    lic = {\n","        \"id\": 1,\n","        \"url\": \"\",\n","        \"name\": \"Unknown\"\n","    }\n","    anns_json[\"licenses\"].append(lic)\n","    classes = {\n","        \"id\": 1, \n","        \"name\": \"starfish\", \n","        \"supercategory\": \"none\"\n","    }\n","    anns_json[\"categories\"].append(classes)\n","    for ann_row in df.itertuples():        \n","        images = {\n","            \"id\": ann_row[0],\n","            \"license\": 1,\n","            \"file_name\": ann_row.image_id + '.jpg',\n","            \"height\": ann_row.height,\n","            \"width\": ann_row.width,\n","            \"date_captured\": \"2021-11-30T15:01:26+00:00\"\n","        }\n","        anns_json[\"images\"].append(images)\n","        bbox_list = ann_row.bboxes\n","        for bbox in bbox_list:\n","            b_width = bbox[2]\n","            b_height = bbox[3]\n","            # some boxes in COTS are outside the image height and width\n","            if (bbox[0] + bbox[2] > 1280):\n","                b_width = 1280 - bbox[0]\n","            if (bbox[1] + bbox[3] > 720):\n","                b_height = 720 - bbox[1]\n","            image_anns = {\n","                \"id\": ann_id,\n","                \"image_id\": ann_row[0],\n","                \"category_id\": 1,\n","                \"bbox\": [bbox[0], bbox[1], b_width, b_height],\n","                \"area\": bbox[2] * bbox[3],\n","                \"segmentation\": [],\n","                \"iscrowd\": 0\n","            }\n","            ann_id += 1\n","            anns_json[\"annotations\"].append(image_anns)\n","    print('COTS annotation to COCO json format done, files:', len(df))\n","    return anns_json"],"id":"V28f2tr6LlIo"},{"cell_type":"code","execution_count":null,"metadata":{"id":"37pVd8YtLlIm"},"outputs":[],"source":["train_path = f'{YDATA_PATH}/train2017'\n","val_path = f'{YDATA_PATH}/val2017'\n","val_fold = (CONFIG['val_video'] - 1) if CONFIG['val_video'] else CONFIG['val_fold']\n","\n","if (not os.path.exists(train_path)) and (not os.path.exists(val_path)):\n","    !mkdir -p {train_path}\n","    !mkdir -p {val_path}\n","    !mkdir -p {YDATA_PATH}/annotations\n","\n","    for i, row in tqdm(df.iterrows(), total=len(df)):\n","        if row.fold != val_fold:\n","            copyfile(\n","                f'{row.image_path}', \n","                f'{train_path}/{row.image_id}.jpg'\n","            )\n","        else:\n","            copyfile(\n","                f'{row.image_path}', \n","                f'{val_path}/{row.image_id}.jpg'\n","            ) \n","    ann_id = 0\n","    train_anns_json = dataset2coco(\n","        df[df.fold != val_fold], \n","        train_path\n","    )\n","    val_anns_json = dataset2coco(\n","        df[df.fold == val_fold], \n","        val_path\n","    )\n","    save_ann_json(\n","        train_anns_json, \n","        f'{YDATA_PATH}/annotations/train.json'\n","    )\n","    save_ann_json(\n","        val_anns_json, \n","        f'{YDATA_PATH}/annotations/val.json'\n","    )\n","\n","print('train files:', len(os.listdir(train_path)))\n","print('val files:', len(os.listdir(val_path)))\n","\n","elapsed_time = time.time() - start_time\n","print(f'time elapsed: {elapsed_time // 60:.0f} min {elapsed_time % 60:.0f} sec')"],"id":"37pVd8YtLlIm"},{"cell_type":"markdown","metadata":{"id":"vcKAQxGILlIp"},"source":["## Config"],"id":"vcKAQxGILlIp"},{"cell_type":"code","execution_count":null,"metadata":{"id":"YW042z1xLlIp"},"outputs":[],"source":["config_file_template = '''\n","#!/usr/bin/env python3\n","# -*- coding:utf-8 -*-\n","# Copyright (c) Megvii, Inc. and its affiliates.\n","\n","import os\n","from yolox.exp import Exp as MyExp\n","\n","class Exp(MyExp):\n","    def __init__(self):\n","        super(Exp, self).__init__()\n","        # ---------------- model config ---------------- #\n","        self.num_classes = $num_classes\n","        self.depth = $depth\n","        self.width = $width\n","        self.act = 'silu'\n","\n","        # ---------------- dataloader config ---------------- #\n","        # set worker to 4 for shorter dataloader init time\n","        self.data_num_workers = $num_workers\n","        self.input_size = $input_size  # (height, width)\n","        # Actual multiscale ranges: [640-5*32, 640+5*32].\n","        # To disable multiscale training, set the\n","        # self.multiscale_range to 0.\n","        # self.multiscale_range = 5\n","        # You can uncomment this line to specify a multiscale range\n","        self.random_size = $random_size\n","        self.data_dir = \"$data_dir\"\n","        self.train_ann = \"train.json\"\n","        self.val_ann = \"val.json\"\n","\n","        # --------------- transform config ----------------- #\n","        self.mosaic_prob = $mosaic_prob\n","        self.mixup_prob = $mixup_prob\n","        self.hsv_prob = $hsv_prob\n","        self.flip_prob = $flip_prob\n","        self.degrees = $degrees\n","        self.translate = $translate\n","        self.mosaic_scale = $mosaic_scale\n","        self.mixup_scale = $mixup_scale\n","        self.shear = $shear\n","        self.enable_mixup = $enable_mixup\n","\n","        # --------------  training config --------------------- #\n","        self.warmup_epochs = $warmup_epochs\n","        self.max_epoch = $max_epoch\n","        self.warmup_lr = 0\n","        self.basic_lr_per_img = $basic_lr_per_img\n","        self.scheduler = \"yoloxwarmcos\"\n","        self.no_aug_epochs = $no_aug_epochs\n","        self.min_lr_ratio = 0.05\n","        self.ema = True\n","\n","        self.weight_decay = 5e-4\n","        self.momentum = 0.9\n","        self.print_interval = 100\n","        self.eval_interval = 1\n","        self.exp_name = os.path.split(os.path.realpath(__file__))[1].split(\".\")[0]\n","\n","        # -----------------  testing config ------------------ #\n","        self.test_size = $test_size\n","        self.test_conf = $test_conf\n","        self.nmsthre = $nmsthre\n","\n","'''\n","print(config_file_template)"],"id":"YW042z1xLlIp"},{"cell_type":"code","execution_count":null,"metadata":{"id":"z905YDp5LlIq"},"outputs":[],"source":[" %cd {WORK_DIR}/YOLOX\n","\n","if CONFIG['bbone'] == 'yolox_s.pth':\n","    depth, width = .33, .50\n","elif CONFIG['bbone'] == 'yolox_m.pth':\n","    depth, width = .67, .75\n","elif CONFIG['bbone'] == 'yolox_l.pth':\n","    depth, width = 1, 1\n","else:\n","    print('ERROR backbone')\n","\n","\n","pipeline = Template(config_file_template).substitute(\n","    num_classes=NUM_CLASSES,\n","    depth=depth,\n","    width=width,\n","    max_epoch=CONFIG['epochs'],\n","    data_dir=YDATA_PATH,\n","    num_workers=CONFIG['workers'],\n","    input_size=CONFIG['resize'],\n","    test_size=(max(CONFIG['resize']), max(CONFIG['resize'])),\n","    test_conf=CONFIG['test_conf'],\n","    nmsthre=CONFIG['nmsthre'],\n","    basic_lr_per_img=CONFIG['basic_lr_per_img'],\n","    random_size=CONFIG['random_size'],\n","    mosaic_prob=CONFIG['mosaic_prob'],\n","    mixup_prob=CONFIG['mixup_prob'],\n","    hsv_prob=CONFIG['hsv_prob'],\n","    flip_prob=CONFIG['flip_prob'],\n","    degrees=CONFIG['degrees'],\n","    translate=CONFIG['translate'],\n","    mosaic_scale=CONFIG['mosaic_scale'],\n","    mixup_scale=CONFIG['mixup_scale'],\n","    shear=CONFIG['shear'],\n","    enable_mixup=CONFIG['enable_mixup'],\n","    warmup_epochs=CONFIG['warmup_epochs'],\n","    no_aug_epochs=CONFIG['no_aug_epochs']\n",")\n","with open(PIPELINE_CONFIG_PATH, 'w') as file:\n","    file.write(pipeline)\n","\n","!cat cots_config_{VER}.py"],"id":"z905YDp5LlIq"},{"cell_type":"code","execution_count":null,"metadata":{"id":"b9nss_0WLlIr"},"outputs":[],"source":["voc_cls = '''\n","VOC_CLASSES = (\n","  \"starfish\",\n",")\n","'''\n","with open('./yolox/data/datasets/voc_classes.py', 'w') as f:\n","    f.write(voc_cls)\n","\n","coco_cls = '''\n","COCO_CLASSES = (\n","  \"starfish\",\n",")\n","'''\n","with open('./yolox/data/datasets/coco_classes.py', 'w') as f:\n","    f.write(coco_cls)\n","\n","!more ./yolox/data/datasets/coco_classes.py"],"id":"b9nss_0WLlIr"},{"cell_type":"markdown","metadata":{"id":"TK3DMnlJLlIr"},"source":["## Train"],"id":"TK3DMnlJLlIr"},{"cell_type":"code","execution_count":null,"metadata":{"id":"cBQ0M98ILlIs"},"outputs":[],"source":["if not os.path.exists(CONFIG['bbone']):\n","    !wget https://github.com/Megvii-BaseDetection/storage/releases/download/0.0.1/{CONFIG['bbone']}\n","!cp ./tools/train.py ./"],"id":"cBQ0M98ILlIs"},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZWrQWBWgLlIs"},"outputs":[],"source":["!python train.py \\\n","    -f cots_config_{VER}.py \\\n","    -d 1 \\\n","    -b {CONFIG['batch_size']} \\\n","    --fp16 \\\n","    -o \\\n","    -c {CONFIG['bbone']}"],"id":"ZWrQWBWgLlIs"},{"cell_type":"code","execution_count":null,"metadata":{"id":"w-7Ow-qNLlIt"},"outputs":[],"source":["!cp {WORK_DIR}/YOLOX/YOLOX_outputs/cots_config_{VER}/best_ckpt.pth {WORK_DIR}/models_{VER}\n","!cp {WORK_DIR}/YOLOX/YOLOX_outputs/cots_config_{VER}/last_epoch_ckpt.pth {WORK_DIR}/models_{VER}\n","!cp {WORK_DIR}/YOLOX/YOLOX_outputs/cots_config_{VER}/train_log.txt {WORK_DIR}/models_{VER}\n","!cp cots_config_{VER}.py {WORK_DIR}/models_{VER}\n","\n","elapsed_time = time.time() - start_time\n","print(f'time elapsed: {elapsed_time // 60:.0f} min {elapsed_time % 60:.0f} sec')"],"id":"w-7Ow-qNLlIt"},{"cell_type":"markdown","metadata":{"id":"Bn7EVjf0LlIt"},"source":["## Inference"],"id":"Bn7EVjf0LlIt"},{"cell_type":"code","execution_count":null,"metadata":{"id":"bqaEqs9SLlIu"},"outputs":[],"source":["from yolox.utils import postprocess\n","from yolox.data.data_augment import ValTransform\n","\n","cots_config = f'cots_config_{VER}'\n","current_exp = importlib.import_module(cots_config)\n","exp = current_exp.Exp()\n","test_size = [int(x * 1) for x in CONFIG['resize']]\n","model = exp.get_model()\n","model.cuda()\n","model.eval()\n","ckpt_file = f'{WORK_DIR}/models_{VER}/best_ckpt.pth'\n","ckpt = torch.load(ckpt_file, map_location='cpu')\n","model.load_state_dict(ckpt[\"model\"])"],"id":"bqaEqs9SLlIu"},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rj5-UnEWLlIu"},"outputs":[],"source":["def yolox_infer(img, model, test_size, th, nms_th, num_classes): \n","    #bboxes = []\n","    #bbclasses = []\n","    #scores = []\n","    preproc = ValTransform(legacy=False)\n","    tensor_img, _ = preproc(img, None, test_size)\n","    tensor_img = torch.from_numpy(tensor_img).unsqueeze(0)\n","    tensor_img = tensor_img.float()\n","    tensor_img = tensor_img.cuda()\n","    with torch.no_grad():\n","        outputs = model(tensor_img)\n","        outputs = postprocess(\n","            outputs, num_classes, th,\n","            nms_th, class_agnostic=True)\n","    if outputs[0] is None:\n","        return torch.Tensor([]), torch.Tensor([]), torch.Tensor([])\n","    outputs = outputs[0].cpu()\n","    bboxes = outputs[:, 0 : 4]\n","    bboxes /= min(test_size[0] / img.shape[0], test_size[1] / img.shape[1])\n","    bbclasses = outputs[:, 6]\n","    scores = outputs[:, 4] * outputs[:, 5]\n","    return bboxes, bbclasses, scores"],"id":"Rj5-UnEWLlIu"},{"cell_type":"code","execution_count":null,"metadata":{"id":"4dTfc4pmLlIu"},"outputs":[],"source":["def draw_yolox_preds(img, bboxes, scores, bbclasses, th, classes_dict):\n","    for i in range(len(bboxes)):\n","        box = bboxes[i]\n","        cls_id = int(bbclasses[i])\n","        score = scores[i]\n","        if score < th:\n","            continue\n","        x0 = int(box[0])\n","        y0 = int(box[1])\n","        x1 = int(box[2])\n","        y1 = int(box[3])\n","        cv2.rectangle(img, (x0, y0), (x1, y1), (0, 255, 0), 2)\n","        cv2.putText(\n","            img, \n","            '{}:{:.1f}%'.format(classes_dict[cls_id], score * 100), \n","            (x0, y0 - 3), \n","            cv2.FONT_HERSHEY_PLAIN, \n","            1.4, \n","            (0, 255, 0), \n","            thickness=2)\n","    return img"],"id":"4dTfc4pmLlIu"},{"cell_type":"code","execution_count":null,"metadata":{"id":"k79hQ6LhLlIv"},"outputs":[],"source":["for id_test in range(10, 16):\n","    img_test = os.listdir(f'{YDATA_PATH}/val2017')[id_test]\n","    TEST_IMAGE_PATH = f'{YDATA_PATH}/val2017/{img_test}'\n","    img = cv2.imread(TEST_IMAGE_PATH)\n","\n","    bboxes, bbclasses, scores = yolox_infer(\n","        img, model, test_size,\n","        TH, NMS_TH, NUM_CLASSES)\n","    out_image = draw_yolox_preds(\n","        img, \n","        bboxes,\n","        scores, \n","        bbclasses, \n","        TH, \n","        COCO_CLASSES)\n","    out_image = cv2.cvtColor(out_image, cv2.COLOR_BGR2RGB)\n","    display(Image.fromarray(out_image))"],"id":"k79hQ6LhLlIv"},{"cell_type":"markdown","metadata":{"id":"baSWgx9NG_bL"},"source":["## Val score"],"id":"baSWgx9NG_bL"},{"cell_type":"code","execution_count":null,"metadata":{"id":"0pMuoUYWJyQi"},"outputs":[],"source":["from typing import List\n","from torchvision.ops import box_iou"],"id":"0pMuoUYWJyQi"},{"cell_type":"code","execution_count":null,"metadata":{"id":"lAEcelsgLlIw"},"outputs":[],"source":["def calculate_score(\n","    preds: List[torch.Tensor],\n","    gts: List[torch.Tensor],\n","    iou_th: float\n",") -> float:\n","    num_tp = 0\n","    num_fp = 0\n","    num_fn = 0\n","    for p, gt in zip(preds, gts):\n","        if len(p) and len(gt):\n","            iou_matrix = box_iou(p, gt)\n","            tp = len(torch.where(iou_matrix.max(0)[0] >= iou_th)[0])\n","            fp = len(p) - tp\n","            fn = len(torch.where(iou_matrix.max(0)[0] < iou_th)[0])\n","            num_tp += tp\n","            num_fp += fp\n","            num_fn += fn\n","        elif len(p) == 0 and len(gt):\n","            num_fn += len(gt)\n","        elif len(p) and len(gt) == 0:\n","            num_fp += len(p)\n","    if (5 * num_tp + 4 * num_fn + num_fp )!=0:\n","        score = 5 * num_tp / (5 * num_tp + 4 * num_fn + num_fp )\n","    else:\n","        score = np.nan\n","    if (num_tp+num_fn) != 0:\n","        recall = num_tp/ (num_tp+num_fn)\n","    else:\n","        recall=np.nan\n","    if (num_tp+num_fp)!=0:\n","        precission = num_tp/ (num_tp+num_fp)\n","    else:\n","        precission=np.nan\n","    return score, precission, recall\n","\n","def evaluate_f2(th, test_size):\n","    scores = []\n","    prec05 = []\n","    rec05 = []\n","    prec03 = []\n","    rec03 = []\n","    iou_ths = np.arange(.3, .85, .05)\n","    for i, row in tqdm(df_val.iterrows(), total=len(df_val), desc=f'th {th}'):\n","        img_path = row.image_path\n","        img = cv2.imread(img_path)\n","        bboxes, bbclasses, scores_ = yolox_infer(\n","            img, model, test_size,\n","            TH, NMS_TH, NUM_CLASSES)\n","        gts =  torch.Tensor([\n","            [x[0], x[1], x[0] + x[2], x[1] + x[3]] \n","            for x in row.bboxes])\n","        bboxes = bboxes[scores_ > th] if scores_.tolist() else bboxes\n","        score = [calculate_score(bboxes.int().unsqueeze(0), gts.unsqueeze(0), iou_th)[0] \n","                 for iou_th in iou_ths]\n","        scores.append(np.nanmean(score))\n","        prec05.append(calculate_score(bboxes.int().unsqueeze(0), gts.unsqueeze(0), .5)[1]) \n","        prec03.append(calculate_score(bboxes.int().unsqueeze(0), gts.unsqueeze(0), .3)[1]) \n","        rec05.append(calculate_score(bboxes.int().unsqueeze(0), gts.unsqueeze(0), .5)[2]) \n","        rec03.append(calculate_score(bboxes.int().unsqueeze(0), gts.unsqueeze(0), .3)[2]) \n","    print(f'threshold {th} |',\n","          f'F2 score: {np.nanmean(scores):.3f} |',\n","          f'precision at .5: {np.nanmean(prec05):.3f}',\n","          f'precision at .3: {np.nanmean(prec03):.3f} |',\n","          f'recall at .5: {np.nanmean(rec05):.3f}',\n","          f'recall at .3: {np.nanmean(rec03):.3f}')\n","    val_results = {\n","        'threshold': th,\n","        'F2 score': np.nanmean(scores),\n","        'precision_at_p5': np.nanmean(prec05),\n","        'precision_at_p3': np.nanmean(prec03),\n","        'recall_at_p5': np.nanmean(rec05),\n","        'recall_at_p3': np.nanmean(rec03)\n","    }\n","    return val_results"],"id":"lAEcelsgLlIw"},{"cell_type":"code","execution_count":null,"metadata":{"id":"S6KlM4jVDTEW"},"outputs":[],"source":["MULT = 1\n","test_size_mult = [int(MULT * x) for x in CONFIG['resize']]\n","\n","df_val = df[df.fold == val_fold]\n","for th in [.1, .3, .5]:\n","    val_results = evaluate_f2(th=th, test_size=test_size_mult)\n","    with open(f'{MDLS_PATH}/best_val_results_th_{th}.json', 'w') as file:\n","        json.dump(val_results, file)"],"id":"S6KlM4jVDTEW"},{"cell_type":"code","execution_count":null,"metadata":{"id":"VypXqEZoDjbL"},"outputs":[],"source":["model = exp.get_model()\n","model.cuda()\n","model.eval()\n","ckpt_file = f'{WORK_DIR}/models_{VER}/last_epoch_ckpt.pth'\n","ckpt = torch.load(ckpt_file, map_location='cpu')\n","model.load_state_dict(ckpt[\"model\"])\n","\n","df_val = df[df.fold == val_fold]\n","for th in [.1, .3, .5]:\n","    val_results = evaluate_f2(th=th, test_size=test_size_mult)\n","    with open(f'{MDLS_PATH}/last_val_results_th_{th}.json', 'w') as file:\n","        json.dump(val_results, file)"],"id":"VypXqEZoDjbL"},{"cell_type":"code","execution_count":null,"metadata":{"id":"GEOkSvxugdHu"},"outputs":[],"source":[""],"id":"GEOkSvxugdHu"}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":[],"machine_shape":"hm","name":"yolox_train.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"papermill":{"default_parameters":{},"duration":11390.68907,"end_time":"2021-12-02T14:52:31.224562","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2021-12-02T11:42:40.535492","version":"2.3.3"}},"nbformat":4,"nbformat_minor":5}